"""
Mini DeepSeek - ç”¨ PyTorch å¾é›¶æ‰“é€ ä¸€å€‹å°å‹çš„ DeepSeek
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import math
import numpy as np

# ==================== 1. è¿·ä½ ç‰ˆ DeepSeek æ¶æ§‹ ====================

class MiniDeepSeek(nn.Module):
    """
    è¿·ä½ ç‰ˆ DeepSeek æ¨¡å‹
    åŒ…å«ï¼š
    - å¤šé ­æ³¨æ„åŠ› (Multi-Head Attention)
    - å‰é¥‹ç¶²è·¯ (Feed Forward)
    - å±¤æ­¸ä¸€åŒ– (Layer Norm)
    - æ®˜å·®é€£æ¥ (Residual Connection)
    """
    
    def __init__(self, vocab_size=10000, d_model=512, n_heads=8, 
                 n_layers=6, d_ff=2048, max_seq_len=1024):
        super().__init__()
        
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.max_seq_len = max_seq_len
        
        # è©åµŒå…¥å±¤
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = self.create_positional_encoding(max_seq_len, d_model)
        
        # Transformer å±¤
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff) 
            for _ in range(n_layers)
        ])
        
        # è¼¸å‡ºå±¤
        self.ln_final = nn.LayerNorm(d_model)
        self.output = nn.Linear(d_model, vocab_size)
        
    def create_positional_encoding(self, max_len, d_model):
        """å»ºç«‹ä½ç½®ç·¨ç¢¼"""
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                            -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        
        return pe
    
    def forward(self, x):
        """
        x: (batch_size, seq_len)
        returns: (batch_size, seq_len, vocab_size)
        """
        seq_len = x.size(1)
        
        # è©åµŒå…¥ + ä½ç½®ç·¨ç¢¼
        x = self.embedding(x) * math.sqrt(self.d_model)
        x = x + self.pos_encoding[:, :seq_len, :].to(x.device)
        
        # é€šé transformer å±¤
        for layer in self.layers:
            x = layer(x)
        
        # è¼¸å‡º
        x = self.ln_final(x)
        logits = self.output(x)
        
        return logits

# ==================== 2. Transformer Block ====================

class TransformerBlock(nn.Module):
    """å–®ä¸€ Transformer å±¤"""
    
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # å¤šé ­æ³¨æ„åŠ›
        self.attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.ln1 = nn.LayerNorm(d_model)
        
        # å‰é¥‹ç¶²è·¯
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.ln2 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # æ³¨æ„åŠ›å­å±¤ + æ®˜å·®é€£æ¥
        attn_output = self.attention(x)
        x = self.ln1(x + self.dropout(attn_output))
        
        # å‰é¥‹å­å±¤ + æ®˜å·®é€£æ¥
        ffn_output = self.ffn(x)
        x = self.ln2(x + self.dropout(ffn_output))
        
        return x

# ==================== 3. å¤šé ­æ³¨æ„åŠ› ====================

class MultiHeadAttention(nn.Module):
    """å¤šé ­æ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # ç·šæ€§æŠ•å½±å±¤
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_k)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.size()
        
        # ç·šæ€§æŠ•å½±ä¸¦åˆ†å‰²æˆå¤šé ­
        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k)
        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k)
        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k)
        
        # è½‰ç½®ä»¥ä¾¿è¨ˆç®—æ³¨æ„åŠ›
        Q = Q.transpose(1, 2)  # (batch, n_heads, seq_len, d_k)
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)
        
        # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        # æ‡‰ç”¨æ³¨æ„åŠ›
        context = torch.matmul(attn, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        # è¼¸å‡ºæŠ•å½±
        output = self.W_o(context)
        
        return output

# ==================== 4. å‰é¥‹ç¶²è·¯ ====================

class FeedForward(nn.Module):
    """å‰é¥‹ç¶²è·¯ (FFN)"""
    
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.GELU()  # ç”¨ GELU æ¿€æ´»
        
    def forward(self, x):
        x = self.linear1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x

# ==================== 5. è³‡æ–™é›† ====================

class TextDataset(Dataset):
    """ç°¡å–®çš„æ–‡å­—è³‡æ–™é›†"""
    
    def __init__(self, texts, tokenizer, max_len=128):
        self.data = []
        for text in texts:
            tokens = tokenizer.encode(text)
            # åˆ‡å‰²æˆå›ºå®šé•·åº¦
            for i in range(0, len(tokens) - max_len, max_len // 2):
                seq = tokens[i:i+max_len]
                if len(seq) < max_len:
                    seq = seq + [0] * (max_len - len(seq))
                self.data.append(torch.tensor(seq))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

# ==================== 6. ç°¡å–®çš„ tokenizer ====================

class SimpleTokenizer:
    """æ¥µç°¡ç‰ˆ tokenizer"""
    
    def __init__(self):
        self.vocab = {}
        self.inv_vocab = {}
        self.vocab_size = 4  # 0: pad, 1: bos, 2: eos, 3: unk
        
    def add_word(self, word):
        if word not in self.vocab:
            self.vocab[word] = self.vocab_size
            self.inv_vocab[self.vocab_size] = word
            self.vocab_size += 1
    
    def encode(self, text):
        """æ–‡å­—è½‰ token"""
        words = text.split()
        tokens = [1]  # bos token
        for word in words:
            if word in self.vocab:
                tokens.append(self.vocab[word])
            else:
                tokens.append(3)  # unk token
        tokens.append(2)  # eos token
        return tokens
    
    def decode(self, tokens):
        """token è½‰æ–‡å­—"""
        words = []
        for t in tokens:
            if t in self.inv_vocab:
                words.append(self.inv_vocab[t])
            elif t == 1:
                words.append('<BOS>')
            elif t == 2:
                words.append('<EOS>')
            else:
                words.append('<UNK>')
        return ' '.join(words)

# ==================== 7. è¨“ç·´å‡½æ•¸ ====================

def train_model(model, dataloader, epochs=10, lr=1e-4):
    """è¨“ç·´æ¨¡å‹"""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥ pad token
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    print(f"ğŸš€ é–‹å§‹è¨“ç·´ (ä½¿ç”¨: {device})")
    print("="*50)
    
    for epoch in range(epochs):
        total_loss = 0
        model.train()
        
        for batch_idx, batch in enumerate(dataloader):
            batch = batch.to(device)
            
            # è¼¸å…¥ = é™¤äº†æœ€å¾Œä¸€å€‹ token
            # ç›®æ¨™ = é™¤äº†ç¬¬ä¸€å€‹ token
            inputs = batch[:, :-1]
            targets = batch[:, 1:].contiguous()
            
            # å‰å‘å‚³æ’­
            optimizer.zero_grad()
            outputs = model(inputs)
            
            # è¨ˆç®—æå¤±
            loss = criterion(
                outputs.view(-1, model.vocab_size),
                targets.view(-1)
            )
            
            # åå‘å‚³æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            
            if (batch_idx + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1} | Loss: {loss.item():.4f}")
        
        avg_loss = total_loss / len(dataloader)
        print(f"ğŸ“Š Epoch {epoch+1} å¹³å‡æå¤±: {avg_loss:.4f}")
        
        # æ¯å€‹ epoch å¾Œç”Ÿæˆä¸€å€‹ç¯„ä¾‹
        if (epoch + 1) % 5 == 0:
            generate_sample(model, tokenizer, device)
    
    return model

# ==================== 8. ç”Ÿæˆæ–‡å­— ====================

def generate_sample(model, tokenizer, device, prompt="<BOS>", max_len=50):
    """ç”Ÿæˆæ–‡å­—ç¯„ä¾‹"""
    
    model.eval()
    
    # å°‡æç¤ºè½‰ç‚º token
    if prompt == "<BOS>":
        input_tokens = [1]
    else:
        input_tokens = tokenizer.encode(prompt)
    
    input_tensor = torch.tensor([input_tokens]).to(device)
    
    with torch.no_grad():
        for _ in range(max_len):
            outputs = model(input_tensor)
            next_token_logits = outputs[0, -1, :]
            
            # å–æœ€é«˜æ©Ÿç‡çš„ token
            next_token = torch.argmax(next_token_logits).item()
            
            if next_token == 2:  # EOS token
                break
            
            input_tensor = torch.cat([
                input_tensor,
                torch.tensor([[next_token]]).to(device)
            ], dim=1)
    
    generated = tokenizer.decode(input_tensor[0].cpu().tolist())
    print(f"\nâœ¨ ç”Ÿæˆ: {generated}\n")

# ==================== 9. ä¸»ç¨‹å¼ ====================

def main():
    """è¨“ç·´ä¸€å€‹è¿·ä½  DeepSeek"""
    
    print("="*60)
    print("ğŸ§  è¿·ä½  DeepSeek è¨“ç·´")
    print("="*60)
    
    # 1. æº–å‚™è³‡æ–™
    tokenizer = SimpleTokenizer()
    
    # åŠ å…¥ä¸€äº›è©å½™
    corpus = [
        "hello world",
        "deep learning is fun",
        "transformer is powerful",
        "I love coding",
        "python is great",
        "neural networks are amazing",
        "artificial intelligence",
        "machine learning",
    ]
    
    # å»ºç«‹è©å½™è¡¨
    for text in corpus:
        for word in text.split():
            tokenizer.add_word(word)
    
    print(f"ğŸ“š è©å½™è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # 2. å»ºç«‹è³‡æ–™é›†
    dataset = TextDataset(corpus, tokenizer, max_len=32)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    # 3. å»ºç«‹æ¨¡å‹ (è¶…è¿·ä½ ç‰ˆ)
    model = MiniDeepSeek(
        vocab_size=tokenizer.vocab_size,
        d_model=128,      # ç¸®å°ç¶­åº¦
        n_heads=4,        # æ¸›å°‘é ­æ•¸
        n_layers=3,       # æ¸›å°‘å±¤æ•¸
        d_ff=256,         # ç¸®å° FFN
        max_seq_len=64
    )
    
    print(f"ğŸ“Š æ¨¡å‹åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # 4. è¨“ç·´
    model = train_model(model, dataloader, epochs=20, lr=1e-4)
    
    # 5. æ¸¬è©¦ç”Ÿæˆ
    print("\n" + "="*60)
    print("ğŸ¯ æ¸¬è©¦ç”Ÿæˆ")
    print("="*60)
    
    test_prompts = [
        "hello",
        "I love",
        "deep",
        "machine",
    ]
    
    for prompt in test_prompts:
        generate_sample(model, tokenizer, torch.device('cpu'), prompt)

# ==================== 10. çœŸæ­£çš„ DeepSeek ç‰¹è‰² ====================

class DeepSeekFeatures:
    """çœŸæ­£çš„ DeepSeek æœ‰çš„ç‰¹è‰²"""
    
    def __init__(self):
        self.features = {
            "1Mä¸Šä¸‹æ–‡": "âœ… æˆ‘å€‘çš„ mini ç‰ˆåªæœ‰ 1024",
            "MoEæ¶æ§‹": "âœ… æˆ‘å€‘ç”¨æ™®é€š FFN",
            "MLAæ³¨æ„åŠ›": "âœ… æˆ‘å€‘ç”¨æ™®é€šå¤šé ­æ³¨æ„åŠ›",
            "FP8é‡åŒ–": "âœ… æˆ‘å€‘ç”¨ FP32",
            "é–‹æºå…è²»": "âœ… é€™å€‹å€’æ˜¯çœŸçš„ï¼",
        }
    
    def compare(self):
        print("\nğŸ“Š è·ŸçœŸæ­£çš„ DeepSeek æ¯”è¼ƒï¼š")
        for feature, status in self.features.items():
            print(f"  {feature}: {status}")

# åŸ·è¡Œ
if __name__ == "__main__":
    main()
    
    # æ¯”è¼ƒ
    DeepSeekFeatures().compare()
    
    print("\n" + "="*60)
    print("ğŸ‰ å®Œæˆï¼ä½ å‰›å‰›è¨“ç·´äº†ä¸€å€‹è¿·ä½  DeepSeekï¼")
    print("   é›–ç„¶å¾ˆå°ï¼Œä½†æ¦‚å¿µæ˜¯ä¸€æ¨£çš„ï¼")
    print("="*60)